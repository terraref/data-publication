{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for running the script\n",
    "# Fill in with your values\n",
    "LOG_DEBUG=False                            # Set to True to enable debugging messages\n",
    "LOG_INFO= True                             # Set to True to enable info messages (overridden by LOG_DEBUG)\n",
    "BETYDB_URL='https://terraref.ncsa.illinois.edu/bety/' # URL to BETYdb instance to use\n",
    "BETYDB_KEY='9999999999999999999999999999999999999999' # Key to use when accessing BETYdb URL\n",
    "BRAPI_URL='https://brapi.workbench.terraref.org/brapi/v1' # Path to BRAPI API to call\n",
    "# Comma separated dates in \"YYYY-MM-DD\". Two dates with a colon ':' are considered a date range\n",
    "#DATES='2017-04-13:2017-09-21'              # Season 4\n",
    "DATES='2018-07-12:2018-07-19'              # Season 6: 2018-04-20:2018-08-02\n",
    "OUTPUT_FILE='season_6_11.db'                  # The name of the SQLite database to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Generates a SQLite database for discovering files\n",
    "\"\"\"\n",
    "import argparse\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "import subprocess\n",
    "import os\n",
    "import stat\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import Callable\n",
    "from typing import Optional\n",
    "import webbrowser\n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "from osgeo import ogr\n",
    "from dateutil.parser import parse\n",
    "\n",
    "LOCAL_START_PATH='/home/jovyan/work/data/terraref/sites/ua-mac'\n",
    "LOCAL_ENVIRONMENT_LOGGER_PATH = 'raw_data/EnvironmentLogger'\n",
    "\n",
    "BETYDB_ENV_URL = 'BETYDB_URL'\n",
    "BETYDB_ENV_KEY = 'BETYDB_KEY'\n",
    "\n",
    "BRAPI_URL = 'https://brapi.workbench.terraref.org/brapi/v1'\n",
    "\n",
    "MAX_INSERT_BEFORE_COMMIT = 1000\n",
    "PLOT_INCLUSION_FILTERS = {'city': 'Maricopa'}\n",
    "\n",
    "TERRAREF_TIMESTAMP_REGEX = '[0-9]{4}-[0-9]{2}-[0-9]{2}__[0-9]{2}-[0-9]{2}-[0-9]{2}-[0-9]{1,3}'\n",
    "\n",
    "# NOTE: SENSOR_MAPS global variable is defined after the mapping functions\n",
    "\n",
    "\n",
    "def local_folder_list(folder_path: str) -> list:\n",
    "    \"\"\"Returns the contents of the folder as a list\n",
    "    Arguments:\n",
    "        folder_path: the path of the folder to search\n",
    "    Returns:\n",
    "        Returns a list of dictionary entries consisting of the following keys:\n",
    "            'name': the name of the file or folder found\n",
    "            'type': one of 'file' or 'dir', with the latter indicating a sub-folder\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    if not os.path.exists(folder_path):\n",
    "        return return_list\n",
    "\n",
    "    for one_name in os.listdir(folder_path):\n",
    "        # Skip over local and parent folder\n",
    "        if one_name == '.' or one_name == '..':\n",
    "            continue\n",
    "        is_file = os.path.isfile(os.path.join(folder_path, one_name))\n",
    "        return_list.append({'name': one_name, 'type': 'file' if is_file else 'dir'})\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def _map_rgb_file_to_metadata(file_directory: str, file_name: str) -> Optional[str]:\n",
    "    \"\"\"Performs mapping of rgb plot level file to associated JSON metadata file\n",
    "    Arguments:\n",
    "        file_directory: the directory of the file\n",
    "        file_name: the name of the file to map\n",
    "    Returns:\n",
    "        The mapped filename or None if the name can't be mapped\n",
    "    \"\"\"\n",
    "    # eg: rgb_geotiff_L1_ua-mac_2018-05-08__13-10-45-826_left.tif\n",
    "    #  -> raw_data/stereoTop/2018-05-08/2018-05-08__13-10-45-826/3a45ac5f-67b5-47c6-a34d-89804269871c_metadata.json\n",
    "    # Note the source file name contains the date string \"2018-05-08__13-10-45-826\"\n",
    "    match = re.search(TERRAREF_TIMESTAMP_REGEX, file_name)\n",
    "    if match:\n",
    "        date = match[0].split('__')[0]\n",
    "        folder_path = os.path.join(LOCAL_START_PATH, 'raw_data/stereoTop', date, match[0])\n",
    "        for one_entry in local_folder_list(folder_path):\n",
    "            if one_entry['name'].endswith('metadata.json'):\n",
    "                return os.path.join(folder_path, one_entry['name'])\n",
    "    return None\n",
    "\n",
    "\n",
    "def _map_ir_file_to_metadata(file_directory: str, file_name: str) -> Optional[str]:\n",
    "    \"\"\"Performs mapping of IR plot level file to associated JSON metadata file\n",
    "    Arguments:\n",
    "        file_directory: the directory of the file\n",
    "        file_name: the name of the file to map\n",
    "    Returns:\n",
    "        The mapped filename or None if the name can't be mapped\n",
    "    \"\"\"\n",
    "    # eg: ir_geotiff_L1_ua-mac_2018-05-19__16-33-12-692.tif\n",
    "    #  -> raw_data/flirIrCamera/2018-05-19/2018-05-19__16-33-12-692//ce65ac12-ee42-4e29-a9eb-17d812b5de7c_metadata.json\n",
    "    # Note the source file name contains the date string \"2018-05-19__16-33-12-692\"\n",
    "    match = re.search(TERRAREF_TIMESTAMP_REGEX, file_name)\n",
    "    if match:\n",
    "        date = match[0].split('__')[0]\n",
    "        folder_path = os.path.join(LOCAL_START_PATH, 'raw_data/flirIrCamera', date, match[0])\n",
    "        for one_entry in local_folder_list(folder_path):\n",
    "            if one_entry['name'].endswith('metadata.json'):\n",
    "                return os.path.join(folder_path, one_entry['name'])\n",
    "    return None\n",
    "\n",
    "\n",
    "def _map_las_file_to_metadata(file_directory: str, file_name: str) -> Optional[str]:\n",
    "    \"\"\"Performs mapping of las plot level file to associated JSON metadata file\n",
    "    Arguments:\n",
    "        file_directory: the directory of the file\n",
    "        file_name: the name of the file to map\n",
    "    Returns:\n",
    "        The mapped filename or None if the name can't be mapped\n",
    "    Exceptions:\n",
    "        Raises RuntimeError if the DTM JSON file can't be found, or can't be downloaded or imported\n",
    "    \"\"\"\n",
    "    # eg: file_directory: /ua-mac/Level_1_Plots/laser3d_las/2018-10-29/MAC Field Scanner Season 7 Range 54 Column 9/\n",
    "    #     file_name: 3d_101118_sstart_fullfield d0aae7fe-e512-4fde-a434-b989fa93f4f9_merged.las\n",
    "\n",
    "    # Get the *merged_dtm.json from Globus\n",
    "    dtm = None\n",
    "    for one_entry in local_folder_list(file_directory):\n",
    "        if one_entry['name'].endswith('merged_dtm.json') and not one_entry['name'].startswith('3d_') and\\\n",
    "        not one_entry['name'].startswith('test_'):\n",
    "            dtm_path = os.path.join(file_directory, one_entry['name'])\n",
    "            if not dtm_path:\n",
    "                raise RuntimeError(\"Unable to retrieve LAS Merged DTM: %s\" % one_entry['name'])\n",
    "            with open(dtm_path, 'r') as in_file:\n",
    "                dtm = json.load(in_file)\n",
    "                break\n",
    "    if dtm is None:\n",
    "        logging.warning(\"Unable to find DTM JSON file associated with '%s'\", os.path.join(file_directory, file_name))\n",
    "        return None\n",
    "\n",
    "    # Get the first items that's a string\n",
    "    timestamp = None\n",
    "    date = None\n",
    "    for key, value in dtm.items():\n",
    "        if isinstance(value, str):\n",
    "            file_name = os.path.basename(value)\n",
    "            if file_name:\n",
    "                match = re.search(TERRAREF_TIMESTAMP_REGEX, file_name)\n",
    "                if match:\n",
    "                    timestamp = match[0]\n",
    "                    date = timestamp.split('__')[0]\n",
    "                    break\n",
    "    if timestamp is None or date is None:\n",
    "        logging.warning(\"Unable to get timestamp associated with file '%s'\", os.path.join(file_directory, file_name))\n",
    "        return None\n",
    "\n",
    "    # Get the path to the metadata JSON file\n",
    "    folder_path = os.path.join(LOCAL_START_PATH, 'raw_data/scanner3DTop', date, timestamp)\n",
    "    for one_entry in local_folder_list(folder_path):\n",
    "        if one_entry['name'].endswith('metadata.json'):\n",
    "            return os.path.join(folder_path, one_entry['name'])\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "SENSOR_MAPS = {\n",
    "    'RGB': {\n",
    "        'file_paths': [\n",
    "            {\n",
    "                'path': 'Level_1_Plots/rgb_geotiff',\n",
    "                'ext': ['tif']\n",
    "            }\n",
    "        ],\n",
    "        'metadata_file_mapper': _map_rgb_file_to_metadata\n",
    "    },\n",
    "    'IR': {\n",
    "        'file_paths': [\n",
    "            {\n",
    "                'path': 'Level_1_Plots/ir_geotiff',\n",
    "                'ext': ['tif']\n",
    "            }\n",
    "        ],\n",
    "        'metadata_file_mapper': _map_ir_file_to_metadata\n",
    "    },\n",
    "    'Lidar': {\n",
    "        'file_paths': [\n",
    "            {\n",
    "                'path': 'Level_1_Plots/laser3d_las',\n",
    "                'ext': ['las'],\n",
    "                'exclude_check': lambda filename: not filename.startswith('3d_')\n",
    "            }\n",
    "        ],\n",
    "        'metadata_file_mapper': _map_las_file_to_metadata\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSORS=','.join(SENSOR_MAPS.keys())       # Change this to a comma separated list of the keys defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_sensors(sensors: str) -> tuple:\n",
    "    \"\"\"Prepares a list of sensors from a comma separated list of sensors\n",
    "    Arguments:\n",
    "        sensors: the comma separated list of sensors\n",
    "    Return:\n",
    "        A tuple of all valid sensors\n",
    "    Exception:\n",
    "        Raises RuntimeError is no valid sensors were found\n",
    "    \"\"\"\n",
    "    sensor_list = []\n",
    "    for one_sensor in sensors.split(','):\n",
    "        cur_sensor = one_sensor.strip()\n",
    "        if cur_sensor in SENSOR_MAPS:\n",
    "            sensor_list.append(cur_sensor)\n",
    "        else:\n",
    "            logging.warning('Unknown sensor specified: %s', cur_sensor)\n",
    "\n",
    "    if not sensor_list:\n",
    "        raise RuntimeError(\"No know sensors were specified on command line\")\n",
    "\n",
    "    return tuple(sensor_list)\n",
    "\n",
    "\n",
    "def validate_date(date: str) -> bool:\n",
    "    \"\"\"Confirms the date passed in is a valid date\n",
    "    Arguments:\n",
    "        date: the date string to confirm\n",
    "    \"\"\"\n",
    "    try:\n",
    "        valid = date == datetime.strptime(date, \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "        if valid:\n",
    "            # Parser throws a ValueError exception if the date's invalid\n",
    "            parse(date)\n",
    "        return valid\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_dates(start_date: str, last_date: str) -> list:\n",
    "    \"\"\"Generates the date strings in the date range\n",
    "    Arguments:\n",
    "        start_date: the expected first date in the range\n",
    "        last_date: the expected last date to include in the range\n",
    "    Return:\n",
    "        Returns a tuple of the valid dates, with the earliest date first\n",
    "    \"\"\"\n",
    "    one_date = parse(start_date)\n",
    "    next_date = parse(last_date)\n",
    "\n",
    "    if one_date < next_date:\n",
    "        first, last = one_date, next_date\n",
    "    else:\n",
    "        first, last = next_date, one_date\n",
    "\n",
    "    all_dates = []\n",
    "    cur_date = first\n",
    "    while cur_date <= last:\n",
    "        all_dates.append(cur_date.strftime(\"%Y-%m-%d\"))\n",
    "        cur_date = cur_date + timedelta(days=1)\n",
    "\n",
    "    return all_dates\n",
    "\n",
    "\n",
    "def prepare_dates(dates_arg: str) -> tuple:\n",
    "    \"\"\"Prepares the dates command line parameter for processing\n",
    "    Arguments:\n",
    "        dates_arg: the command line parameter value\n",
    "    Return:\n",
    "        Returns an expanded list of dates to include\n",
    "    Exceptions:\n",
    "        RuntimeError is raised if a problem is found\n",
    "    \"\"\"\n",
    "    all_dates = dates_arg.split(',')\n",
    "    if not all_dates:\n",
    "        raise RuntimeError(\"Dates parameter is missing values\")\n",
    "\n",
    "    dates = []\n",
    "    problems = False\n",
    "    for one_item in all_dates:\n",
    "        # Check for a single date or a date range\n",
    "        first_date = one_item\n",
    "        last_date = one_item\n",
    "        if ':' in one_item:\n",
    "            first_date, last_date = one_item.split(':')\n",
    "\n",
    "        # Determine if we have a single date or a range\n",
    "        if first_date == last_date:\n",
    "            cur_date = first_date.strip()\n",
    "            if cur_date:\n",
    "                if validate_date(cur_date):\n",
    "                    dates.append(cur_date)\n",
    "                else:\n",
    "                    logging.warning(\"Invalid date specified: '%s'\", cur_date)\n",
    "                    problems = True\n",
    "                    continue\n",
    "        else:\n",
    "            cur_start = first_date.strip()\n",
    "            cur_last = last_date.strip()\n",
    "            if not cur_start or not cur_last:\n",
    "                logging.warning(\"Invalid date range specified: '%s'\", one_item)\n",
    "                problems = True\n",
    "                continue\n",
    "            if not validate_date(cur_start) or not validate_date(cur_last):\n",
    "                logging.warning(\"Invalid dates specified in date range: '%s'\", one_item)\n",
    "                problems = True\n",
    "                continue\n",
    "            dates.extend(generate_dates(cur_start, cur_last))\n",
    "\n",
    "    if problems:\n",
    "        raise RuntimeError(\"Errors found while processing command line dates. Please correct and try again\")\n",
    "\n",
    "    return tuple(dates)\n",
    "\n",
    "\n",
    "def get_betydb_url(betydb_url_arg: str) -> str:\n",
    "    \"\"\"Returns the BETYdb URL\n",
    "    Arguments:\n",
    "        betydb_url_arg: the command line argument for the BETYdb URL\n",
    "    Return:\n",
    "        Returns the found BETYdb URL\n",
    "    \"\"\"\n",
    "    if betydb_url_arg and betydb_url_arg.strip():\n",
    "        return betydb_url_arg\n",
    "\n",
    "    env_url = os.environ.get(BETYDB_ENV_URL)\n",
    "    if not env_url:\n",
    "        logging.warning(\"BETYDB_URL environment variable has not been set\")\n",
    "\n",
    "    return env_url\n",
    "\n",
    "\n",
    "def get_betydb_key(betydb_key_arg: str) -> str:\n",
    "    \"\"\"Returns the BETYdb key used to access the URL\n",
    "    Arguments:\n",
    "        betydb_key_arg: the command line argument for the BETYdb key\n",
    "    Return:\n",
    "        Returns the found BETYdb key\n",
    "    \"\"\"\n",
    "    if betydb_key_arg and betydb_key_arg.strip():\n",
    "        return betydb_key_arg\n",
    "\n",
    "    env_key = os.environ.get(BETYDB_ENV_KEY)\n",
    "    if not env_key:\n",
    "        logging.warning(\"BETYDB_KEY environment variable has not been set\")\n",
    "\n",
    "    return env_key\n",
    "\n",
    "\n",
    "def get_brapi_url(brapi_url_arg: str) -> str:\n",
    "    \"\"\"Returns the BRAPI URL to use when fetching data\n",
    "    Arguments:\n",
    "        brapi_url_arg: the command line argument for the BRAPI URL\n",
    "    Return:\n",
    "        Returns the BRAPI URL to use\n",
    "    \"\"\"\n",
    "    if brapi_url_arg and brapi_url_arg.strip():\n",
    "        return brapi_url_arg\n",
    "\n",
    "    return BRAPI_URL\n",
    "\n",
    "\n",
    "def make_timestamp_instance(timestamp_string: str) -> datetime:\n",
    "    \"\"\"Converts a string timestamp to a timestamp object\n",
    "    Arguments:\n",
    "        timestamp_string: the timestamp to convert (see Notes)\n",
    "    Return:\n",
    "        Returns a timestamp object representing the timestamp passed in\n",
    "    Notes:\n",
    "        Only accepts timestamp strings with the following format:\n",
    "            \"MM/DD/YYYY HH:MI:SS\"\n",
    "            \"YYYY.MM.DD-HH:MI:SS\"\n",
    "    \"\"\"\n",
    "    if '.' in timestamp_string:\n",
    "        return datetime.strptime(timestamp_string, '%Y.%m.%d-%H:%M:%S')\n",
    "\n",
    "    return datetime.strptime(timestamp_string, '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "def get_experiments_by_dates(dates: tuple, betydb_url: str, betydb_key: str, experiment_json_file: str = None) -> tuple:\n",
    "    \"\"\"Retrieves the experiments associated with dates\n",
    "    Arguments:\n",
    "        dates: the dates to fetch experiment information on\n",
    "        betydb_url: the URL to the BETYdb instance to query\n",
    "        betydb_key: the key to use in association with the BETYdb URL\n",
    "        experiment_json_file: optional path to json file containing experiment data from BETYdb\n",
    "    Return:\n",
    "        A tuple containing the list of experiments matching the dates, a list of dates with their associated experiment\n",
    "        ID, and a list of dates for which experiments were NOT found\n",
    "    \"\"\"\n",
    "    found_experiments = []\n",
    "    date_experiment_id = {}\n",
    "    remaining_dates = dates\n",
    "\n",
    "    # Get experiments JSON\n",
    "    if not experiment_json_file or not os.path.exists(experiment_json_file):\n",
    "        query_params = {'key': betydb_key, 'limit': 'none', 'associations_mode': 'full_info'}\n",
    "\n",
    "        # Get the experiments and find matches\n",
    "        url = os.path.join(betydb_url, 'api/v1/experiments')\n",
    "        result = requests.get(url, params=query_params, verify=False)\n",
    "        result.raise_for_status()\n",
    "\n",
    "        result_json = result.json()\n",
    "    else:\n",
    "        with open(experiment_json_file, \"r\") as in_file:\n",
    "            result_json = json.load(in_file)\n",
    "    if 'data' in result_json:\n",
    "        experiments = result_json['data']\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid format of returned experiment JSON (missing 'data' key)\")\n",
    "\n",
    "    # Find the ones that match our dates\n",
    "    for one_exp in experiments:\n",
    "        exp_data = one_exp['experiment']\n",
    "        # This is inefficient; it'd be better to keep the date ranges for comparison and not expand them\n",
    "        exp_dates = generate_dates(exp_data['start_date'], exp_data['end_date'])\n",
    "        date_matches = tuple(set(exp_dates).intersection(set(remaining_dates)))\n",
    "\n",
    "        if date_matches:\n",
    "            found_experiments.append(exp_data)\n",
    "            remaining_dates = tuple(set(remaining_dates) - set(exp_dates))\n",
    "            for one_date in date_matches:\n",
    "                date_experiment_id[one_date] = exp_data['id']\n",
    "\n",
    "    return found_experiments, date_experiment_id, remaining_dates\n",
    "\n",
    "\n",
    "def get_cultivars_betydb(betydb_url: str, betydb_key: str, cultivar_json_file: str = None) -> list:\n",
    "    \"\"\"Retrieves all the cultivars from BETYdb\n",
    "    Arguments:\n",
    "        betydb_url: the URL to the BETYdb instance to query\n",
    "        betydb_key: the key to use in association with the BETYdb URL\n",
    "        cultivar_json_file: optional path to json file containing cultivar data from BETYdb\n",
    "    Return:\n",
    "        Returns the result of the query\n",
    "    \"\"\"\n",
    "    if not cultivar_json_file or not os.path.exists(cultivar_json_file):\n",
    "        query_params = {'key': betydb_key, 'limit': 'none', 'associations_mode': 'full_info'}\n",
    "\n",
    "        # Get the cultivators\n",
    "        url = os.path.join(betydb_url, 'api/v1/cultivars')\n",
    "        result = requests.get(url, params=query_params, verify=False)\n",
    "        result.raise_for_status()\n",
    "\n",
    "        result_json = result.json()\n",
    "    else:\n",
    "        with open(cultivar_json_file, 'r') as in_file:\n",
    "            result_json = json.load(in_file)\n",
    "    if 'data' in result_json:\n",
    "        return result_json['data']\n",
    "\n",
    "    raise RuntimeError(\"Invalid format of returned cultivar JSON (missing 'data' key)\")\n",
    "\n",
    "\n",
    "def get_cultivars_brapi(study_id: str, brapi_url: str) -> list:\n",
    "    \"\"\"Retrieves cultivar information from BRAPI on a per study basis\n",
    "    Arguments:\n",
    "        study_id: the ID of the study (experiment in BETYdb terms)\n",
    "        brapi_url: the base BRAPI URL to use when making calls\n",
    "    Returns:\n",
    "        Returns the list of results containing the information on the study\n",
    "    Notes:\n",
    "        Will make calls until all pages of data are returned for the study\n",
    "    \"\"\"\n",
    "    base_url = os.path.join(brapi_url, 'studies', str(study_id), 'layouts')\n",
    "    params = {'page': -1}   # Start at -1 since we pre-increment before making a call\n",
    "    studies_data = []\n",
    "\n",
    "    # Loop through until we're done\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Making the call to get the data\n",
    "        params['page'] += 1\n",
    "        response = requests.get(base_url, params, verify=False)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Getting and handling the response\n",
    "        response_json = response.json()\n",
    "        if not response_json:\n",
    "            logging.warning(\"Received an empty JSON response from BRAPI studies. Stopping fetch of studies\")\n",
    "            done = True\n",
    "            continue\n",
    "\n",
    "        if 'result' not in response_json or 'data' not in response_json['result']:\n",
    "            logging.warning(\"Unknown JSON format received from BRAPI studies request. Stopping fetch of studies\")\n",
    "            done = True\n",
    "            continue\n",
    "        if not isinstance(response_json['result']['data'], list):\n",
    "            logging.warning(\"BRAPI studies request returned unexpected non-list data type result. Stopping fetch of studies\")\n",
    "            done = True\n",
    "            continue\n",
    "\n",
    "        # Merge the data or indicate we are done (due to an empty result)\n",
    "        if response_json['result']['data']:\n",
    "            studies_data.extend(response_json['result']['data'])\n",
    "        else:\n",
    "            done = True\n",
    "\n",
    "    return studies_data\n",
    "\n",
    "\n",
    "def match_cultivar_to_site_betydb(site_id: int, all_cultivars: list) -> Optional[tuple]:\n",
    "    \"\"\"Finds the cultivar that matches the site ID\n",
    "    Arguments:\n",
    "        site_id: the ID of the site of interest\n",
    "        all_cultivars: the list of available cultivars\n",
    "    Return:\n",
    "        A tuple containing the found cultivar (dict) and the site trait (dict). None is returned if the site ID can't\n",
    "        be matched\n",
    "    \"\"\"\n",
    "    site_id_str = str(site_id)\n",
    "\n",
    "    for one_cultivar in all_cultivars:\n",
    "        if 'cultivar' in one_cultivar and 'traits' in one_cultivar['cultivar']:\n",
    "            for one_trait in one_cultivar['cultivar']['traits']:\n",
    "                if 'trait' in one_trait and 'site_id' in one_trait['trait']:\n",
    "                    if one_trait['trait']['site_id'] == site_id_str:\n",
    "                        # Return the found item\n",
    "                        return one_cultivar, one_trait\n",
    "\n",
    "    logging.warning(\"Didn't find a cultivar for site: %s\", site_id_str)\n",
    "    return None\n",
    "\n",
    "\n",
    "def match_cultivar_to_site_brapi(site_id: int, all_cultivars: list) -> Optional[dict]:\n",
    "    \"\"\"Finds the cultivar that matches the site ID\n",
    "    Arguments:\n",
    "        site_id: the ID of the site of interest\n",
    "        all_cultivars: the list of available cultivars\n",
    "    Return:\n",
    "        A tuple containing the found cultivar (dict) and the site trait (dict). None is returned if the site ID can't\n",
    "        be matched\n",
    "    \"\"\"\n",
    "    site_id_str = str(site_id)\n",
    "\n",
    "    for one_cultivar in all_cultivars:\n",
    "        if 'observationUnitDbId' in one_cultivar:\n",
    "            if one_cultivar['observationUnitDbId'] == site_id_str:\n",
    "                # Return the found item\n",
    "                return one_cultivar\n",
    "\n",
    "    logging.debug(\"Didn't find a cultivar for site: %s\", site_id_str)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_bounds_from_wkt(wkt: str) -> tuple:\n",
    "    \"\"\"Returns the bounds represented by the WKT (Well Known Text) geometry representation\n",
    "    Arguments:\n",
    "        wkt: the well know text to return the bounds of\n",
    "    Return:\n",
    "        A tuple containing the minimum latitude (Y), minimum longitude (X), maximum latitude (Y), maximum longitude (X) of the\n",
    "        geometry's bounding box\n",
    "    Exceptions:\n",
    "        Raises a RuntimeError if a problem is found\n",
    "    \"\"\"\n",
    "    geometry = ogr.CreateGeometryFromWkt(wkt)\n",
    "    if not geometry:\n",
    "        raise RuntimeError(\"Unable to convert WKT to a working geometry: '%s'\" % wkt)\n",
    "\n",
    "    envelope = geometry.GetEnvelope()\n",
    "    return envelope[2], envelope[0], envelope[3], envelope[1]\n",
    "\n",
    "\n",
    "def get_save_experiments(dates: tuple, db_conn: sqlite3.Connection, betydb_url: str, betydb_key: str,\n",
    "                         brapi_url: str, experiment_json_file: str = None, cultivar_json_file: str = None) -> Optional[tuple]:\n",
    "    \"\"\"Retrieves the experiments associated with the dates and saves them into the database\n",
    "    Arguments:\n",
    "        dates: the dates to fetch experiment information on\n",
    "        db_conn: the database to write to\n",
    "        betydb_url: the URL to the BETYdb instance to query\n",
    "        betydb_key: the key to use in association with the BETYdb URL\n",
    "        brapi_url: the BRAPI URL to fetch data from\n",
    "        experiment_json_file: optional path to json file containing experiment data from BETYdb\n",
    "        cultivar_json_file: optional path to json file containing cultivar data from BETYDB\n",
    "    Return:\n",
    "        A tuple consisting of the list of experiments saved to the SQLite database, a list of their associated cultivars,\n",
    "        and a dictionary of dates with their associated experiment IDs\n",
    "    Exceptions:\n",
    "        A RuntimeError exception is raised when problems are found\n",
    "    \"\"\"\n",
    "    # Get the experiments\n",
    "    found_experiments, date_experiment_ids, remaining_dates = get_experiments_by_dates(dates, betydb_url, betydb_key,\n",
    "                                                                                       experiment_json_file)\n",
    "\n",
    "    # Report any left over dates outside of experiments\n",
    "    if remaining_dates:\n",
    "        logging.warning(\"Unable to find experiments for all dates and date ranges specified: %s\", ','.join(remaining_dates))\n",
    "    if not found_experiments:\n",
    "        logging.error(\"No experiments were found for the requested dates\")\n",
    "        return None\n",
    "\n",
    "    # Get the cultivars\n",
    "    # all_cultivars = get_cultivars_betydb(betydb_url, betydb_key, cultivar_json_file)\n",
    "    all_cultivars = {}\n",
    "    for one_experiment in found_experiments:\n",
    "        all_cultivars[one_experiment['id']] = get_cultivars_brapi(one_experiment['id'], brapi_url)\n",
    "        logging.debug(\"Retrieved %s BRAPI cultivar entries for Experiment: %s\", str(len(all_cultivars[one_experiment['id']])),\n",
    "                      str(one_experiment['name']))\n",
    "\n",
    "    # Create the experiments table\n",
    "    exp_cursor = db_conn.cursor()\n",
    "    exp_cursor.execute('''CREATE TABLE season_info\n",
    "                          (id INTEGER, plot_name TEXT, season_id INTEGER, season TEXT, cultivar_id INTEGER, \n",
    "                          plot_bb_min_lat FLOAT, plot_bb_min_lon FLOAT, plot_bb_max_lat FLOAT, plot_bb_max_lon FLOAT)''')\n",
    "\n",
    "    # Insert the data and commit every so often\n",
    "    problem_found = False\n",
    "    num_inserted = 0\n",
    "    total_records = 0\n",
    "    cultivars_matched = []\n",
    "    for found_exp in found_experiments:\n",
    "        for one_site in found_exp['sites']:\n",
    "            # Check for any inclusion filters\n",
    "            cur_site = one_site['site']\n",
    "            if PLOT_INCLUSION_FILTERS:\n",
    "                inclusion_match = True\n",
    "                for key in PLOT_INCLUSION_FILTERS.keys():\n",
    "                    if key not in cur_site:\n",
    "                        inclusion_match = False\n",
    "                        break\n",
    "                    if cur_site[key] != PLOT_INCLUSION_FILTERS[key]:\n",
    "                        inclusion_match = False\n",
    "                        break\n",
    "                if not inclusion_match:\n",
    "                    logging.debug(\"Filtering out site '%s'\", str(cur_site['id']))\n",
    "                    continue\n",
    "\n",
    "            # Find out cultivar\n",
    "            cultivar_match = match_cultivar_to_site_brapi(cur_site['id'], all_cultivars[found_exp['id']])\n",
    "            if not cultivar_match:\n",
    "                logging.warning(\"Unable to find matching cultivar for site: '%s'\", str(cur_site))\n",
    "                problem_found = True\n",
    "                continue\n",
    "\n",
    "            # Add our cultivar in if we don't have it yet\n",
    "            already_added = False\n",
    "            for one_cultivar in cultivars_matched:\n",
    "                if one_cultivar['germPlasmDbId'] == cultivar_match['germPlasmDbId']:\n",
    "                    already_added = True\n",
    "                    break\n",
    "            if not already_added:\n",
    "                cultivars_matched.append(cultivar_match)\n",
    "\n",
    "            # Get our plot bounding points\n",
    "            min_lat, min_lon, max_lat, max_lon = get_bounds_from_wkt(cur_site['geometry'])\n",
    "\n",
    "            if 'sitename' in cur_site:\n",
    "                site_name = cur_site['sitename']\n",
    "            else:\n",
    "                site_name = \"unknown %s\" % str(cur_site['id'])\n",
    "\n",
    "            exp_cursor.execute(\"INSERT INTO season_info VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                               [cur_site['id'], site_name, found_exp['id'], found_exp['name'], cultivar_match['germPlasmDbId'],\n",
    "                                min_lat, min_lon, max_lat, max_lon])\n",
    "\n",
    "            num_inserted += 1\n",
    "            total_records += 1\n",
    "            if num_inserted >= MAX_INSERT_BEFORE_COMMIT:\n",
    "                db_conn.commit()\n",
    "                num_inserted = 0\n",
    "\n",
    "    # Create an index\n",
    "    exp_cursor.execute(\"CREATE UNIQUE INDEX 'season_info_index' on 'season_info' ('id', 'cultivar_id' asc)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    exp_cursor.close()\n",
    "\n",
    "    # Handle problems\n",
    "    if problem_found:\n",
    "        raise RuntimeError(\"Problems found processing experiments - unable to continue\")\n",
    "    if total_records <= 0:\n",
    "        logging.warning(\"No experiments records were written\")\n",
    "\n",
    "    logging.debug(\"Wrote %s experiments records\", str(total_records))\n",
    "    return found_experiments, cultivars_matched, date_experiment_ids\n",
    "\n",
    "\n",
    "def save_cultivars(cultivars: list, db_conn: sqlite3.Connection) -> None:\n",
    "    \"\"\"Saves the cultivars to the database\n",
    "    Arguments:\n",
    "        cultivars: the list of cultivars to save\n",
    "        db_conn: the database to write to\n",
    "    \"\"\"\n",
    "    # Create the cultivars table\n",
    "    cult_cursor = db_conn.cursor()\n",
    "    cult_cursor.execute('''CREATE TABLE cultivars\n",
    "                          (id INTEGER, name TEXT)''')\n",
    "\n",
    "    # Write to the table\n",
    "    num_inserted = 0\n",
    "    total_records = 0\n",
    "    for one_cultivar in cultivars:\n",
    "        cult_cursor.execute(\"INSERT INTO cultivars VALUES(?, ?)\", [one_cultivar['germPlasmDbId'], one_cultivar['germplasmName']])\n",
    "\n",
    "        num_inserted += 1\n",
    "        total_records += 1\n",
    "        if num_inserted >= MAX_INSERT_BEFORE_COMMIT:\n",
    "            db_conn.commit()\n",
    "            num_inserted = 0\n",
    "\n",
    "    # Create an index\n",
    "    cult_cursor.execute(\"CREATE UNIQUE INDEX 'cultivars_index' on 'cultivars' ('id', 'name' asc)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    cult_cursor.close()\n",
    "\n",
    "    if total_records <= 0:\n",
    "        logging.warning(\"No cultivar records were written\")\n",
    "    logging.debug(\"Wrote %s cultivar records\", str(total_records))\n",
    "\n",
    "\n",
    "def local_get_files_info(files_path: str, extensions: list, metadata_file_mapper: Callable,\n",
    "                          filename_check: Optional[Callable]) -> Optional[list]:\n",
    "    \"\"\"Loads the files found on the path and returns their information\n",
    "    Arguments:\n",
    "        files_path: the path to load file information from\n",
    "        extensions: a list of acceptable filename extensions (can be wildcard '*')\n",
    "        metadata_file_mapper: function to map a file name to its metadata file\n",
    "        filename_check: optional function for checking whether a filename is acceptable\n",
    "    Return:\n",
    "        Returns a list of files associated with the file path\n",
    "    \"\"\"\n",
    "    file_details = []\n",
    "    json_file = None\n",
    "\n",
    "    # Load all the files in the folder that are filtered in by extension, or are metadata JSON\n",
    "    for one_entry in local_folder_list(files_path):\n",
    "        # Check if we have a filtering function and use it if we do\n",
    "        if filename_check:\n",
    "            if not filename_check(one_entry['name']):\n",
    "                continue\n",
    "\n",
    "        # Get the format of the file (aka: its extension)\n",
    "        file_format = os.path.splitext(one_entry['name'])[1]\n",
    "        if file_format:\n",
    "            file_format = file_format.lstrip('.')\n",
    "\n",
    "        # Check for extension matching (we always keep metadata JSON files)\n",
    "        match_found = one_entry['name'].endswith('_metadata.json')\n",
    "        for one_ext in extensions:\n",
    "            if one_ext in ('*', file_format):\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            logging.debug(\"Skipping over file due to non-matching extension: %s\", one_entry['name'])\n",
    "            continue\n",
    "\n",
    "        # Prepare the file information\n",
    "        file_info = {\n",
    "            'directory': files_path,\n",
    "            'filename': one_entry['name'],\n",
    "            'format': file_format\n",
    "        }\n",
    "        file_details.append(file_info)\n",
    "\n",
    "        if one_entry['name'].endswith('metadata.json'):\n",
    "            json_file = one_entry['name']\n",
    "\n",
    "    # If we don't have anything, return nothing\n",
    "    if not file_details:\n",
    "        return None\n",
    "\n",
    "    # Fill in each file's json file entry\n",
    "    missing_json_files = False\n",
    "    for one_file in file_details:\n",
    "        if one_file['filename'].endswith('_metadata.json'):\n",
    "            continue\n",
    "\n",
    "        if not json_file:\n",
    "            if metadata_file_mapper:\n",
    "                logging.debug(\"Calling metadata file mapper with: '%s' '%s'\", one_file['directory'], one_file['filename'])\n",
    "                json_file = metadata_file_mapper(one_file['directory'], one_file['filename'])\n",
    "            if not json_file:\n",
    "                missing_json_files = True\n",
    "                logging.info(\"Unable to find JSON file for file %s\", os.path.join(one_file['directory'], one_file['filename']))\n",
    "        if json_file:\n",
    "            one_file['json_file'] = json_file\n",
    "\n",
    "    if missing_json_files:\n",
    "        logging.warning(\"Missing metadata JSON files\")\n",
    "\n",
    "    return file_details\n",
    "\n",
    "\n",
    "def local_get_files_details(date_files_info: dict, json_file_list: list) -> Optional[dict]:\n",
    "    \"\"\"Gets the details of the files in the list\n",
    "    Arguments:\n",
    "        date_files_info: list of file information\n",
    "        json_file_list: the list of JSON files to fetch\n",
    "    Return:\n",
    "        Returns an updated list of file details\n",
    "    \"\"\"\n",
    "    # Fetch metadata and pull information out of it\n",
    "    return_info = {}\n",
    "    for one_date, file_list in date_files_info.items():\n",
    "        return_info[one_date] = []\n",
    "        for one_file in file_list:\n",
    "            if 'json_file' not in one_file:\n",
    "                logging.debug(\"   No loading details for file with no json: %s\", one_file['filename'])\n",
    "                return_info[one_date].append(one_file)\n",
    "                continue\n",
    "\n",
    "            variable_metadata = {}\n",
    "            fixed_metadata = {}\n",
    "            local_path = one_file['json_file']\n",
    "            logging.debug(\"Loading JSON file %s for file %s\", local_path, one_file['filename'])\n",
    "            with open(local_path, 'r') as in_file:\n",
    "                metadata = json.load(in_file)\n",
    "                if 'lemnatec_measurement_metadata' in metadata:\n",
    "                    lmm = metadata['lemnatec_measurement_metadata']\n",
    "                    for one_key in ['gantry_system_variable_metadata', 'sensor_variable_metadata']:\n",
    "                        if one_key in lmm:\n",
    "                            variable_metadata[one_key] = lmm[one_key]\n",
    "                    for one_key in ['gantry_system_fixed_metadata', 'sensor_fixed_metadata']:\n",
    "                        if one_key in lmm:\n",
    "                            fixed_metadata[one_key] = lmm[one_key]\n",
    "\n",
    "            pos_x, pos_y, pos_z, start_time = None, None, None, None\n",
    "            if 'gantry_system_variable_metadata' in variable_metadata:\n",
    "                gsvm = variable_metadata['gantry_system_variable_metadata']\n",
    "                if 'position x [m]' in gsvm:\n",
    "                    pos_x = gsvm['position x [m]']\n",
    "                if 'position y [m]' in gsvm:\n",
    "                    pos_y = gsvm['position y [m]']\n",
    "                if 'position z [m]' in gsvm:\n",
    "                    pos_z = gsvm['position z [m]']\n",
    "                if 'time' in gsvm:\n",
    "                    start_time = gsvm['time']\n",
    "\n",
    "            # Update the file information\n",
    "            more_details = {'local_json_file': local_path}\n",
    "            if variable_metadata:\n",
    "                more_details['variable_metadata'] = variable_metadata\n",
    "            if fixed_metadata:\n",
    "                more_details['fixed_metadata'] = fixed_metadata\n",
    "            if pos_x:\n",
    "                more_details['gantry_x'] = pos_x\n",
    "            if pos_y:\n",
    "                more_details['gantry_y'] = pos_y\n",
    "            if pos_z:\n",
    "                more_details['gantry_z'] = pos_z\n",
    "            if start_time:\n",
    "                more_details['start_time'] = start_time\n",
    "                more_details['finish_time'] = start_time\n",
    "\n",
    "            return_info[one_date].append({**more_details, **one_file})\n",
    "\n",
    "    return return_info\n",
    "\n",
    "\n",
    "def local_get_files(local_folder: str, sensor_path: str, extensions: list, date_experiment_ids: dict,\n",
    "                     metadata_file_mapper: Callable, filename_check: Optional[Callable]) -> dict:\n",
    "    \"\"\"Returns a list of files on the endpoint path that match the dates provided\n",
    "    Arguments:\n",
    "        local_folder: the local folder to access files from\n",
    "        sensor_path: the sensor specific path\n",
    "        extensions: a list of acceptable filename extensions (can be wildcard '*')\n",
    "        date_experiment_ids: dates with their associated experiment ID\n",
    "        metadata_file_mapper: function to map a file name to its metadata file\n",
    "        filename_check: optional function for checking whether a filename is acceptable\n",
    "    Return:\n",
    "        Returns a dictionary with dates as keys, each associated with a list of informational dict's on the files found\n",
    "    \"\"\"\n",
    "    found_files = {}\n",
    "    working_file_set = {}\n",
    "    download_file_list = []\n",
    "    base_path = os.path.join(local_folder, sensor_path)\n",
    "    for one_date in date_experiment_ids.keys():\n",
    "        cur_path = os.path.join(base_path, one_date)\n",
    "        logging.info(\"Local path: %s\", cur_path)\n",
    "        path_contents = None\n",
    "        path_contents = local_folder_list(cur_path)\n",
    "\n",
    "        for one_entry in path_contents:\n",
    "            if one_entry['type'] == 'dir':\n",
    "                sub_path = os.path.join(cur_path, one_entry['name'])\n",
    "                logging.debug(\"Local file path: %s\", sub_path)\n",
    "                cur_files = local_get_files_info(sub_path, extensions, metadata_file_mapper, filename_check)\n",
    "                if cur_files:\n",
    "                    logging.debug(\"Found %s files for sub path: %s with extensions %s\", str(len(cur_files)), sub_path, str(extensions))\n",
    "                    if one_date not in working_file_set:\n",
    "                        working_file_set[one_date] = cur_files\n",
    "                    else:\n",
    "                        working_file_set[one_date].extend(cur_files)\n",
    "\n",
    "                    for one_file in cur_files:\n",
    "                        if 'json_file' in one_file:\n",
    "                            if one_file['json_file'] not in download_file_list:\n",
    "                                download_file_list.append(os.path.join(one_file['directory'], one_file['json_file']))\n",
    "                else:\n",
    "                    logging.debug(\"Found 0 files for sub path: %s\", sub_path)\n",
    "\n",
    "            # Only download files when we have a group of them\n",
    "            if len(download_file_list) >= 10:\n",
    "                logging.debug(\"Have 100 files to download - getting file details\")\n",
    "                new_details = local_get_files_details(working_file_set, download_file_list)\n",
    "                for cur_date in new_details:\n",
    "                    if cur_date not in found_files:\n",
    "                        found_files[cur_date] = new_details[cur_date]\n",
    "                    else:\n",
    "                        found_files[cur_date].extend(new_details[cur_date])\n",
    "                working_file_set = {}\n",
    "                download_file_list = []\n",
    "\n",
    "    if len(download_file_list) > 0:\n",
    "        logging.debug(\"Have %s remaining files to download - getting file details\", str(len(download_file_list)))\n",
    "        new_details = local_get_files_details(working_file_set, download_file_list)\n",
    "        for cur_date in new_details:\n",
    "            if cur_date not in found_files:\n",
    "                found_files[cur_date] = new_details[cur_date]\n",
    "            else:\n",
    "                found_files[cur_date].extend(new_details[cur_date])\n",
    "\n",
    "    return found_files\n",
    "\n",
    "\n",
    "def map_file_to_plot_id(file_path: str, season_id: str, seasons: list) -> str:\n",
    "    \"\"\"Find the plot that is associated with the file\n",
    "    Arguments:\n",
    "        file_path: the path to the file\n",
    "        season_id: the ID of the season associated with the file\n",
    "        seasons: the list of seasons\n",
    "    Return:\n",
    "        Returns the found plot ID\n",
    "    Exceptions:\n",
    "        Raises RuntimeError if the plot ID isn't found\n",
    "    \"\"\"\n",
    "    found_plot_id = None\n",
    "    file_parts = file_path.split('/')\n",
    "    for one_season in seasons:\n",
    "        if 'id' not in one_season or 'sites' not in one_season or not one_season['id'] == season_id:\n",
    "            continue\n",
    "        for one_site in one_season['sites']:\n",
    "            if 'site' not in one_site or 'sitename' not in one_site['site']:\n",
    "                continue\n",
    "            if one_site['site']['sitename'] in file_parts:\n",
    "                found_plot_id = one_site['site']['id']\n",
    "                break\n",
    "\n",
    "    if found_plot_id is None:\n",
    "        raise RuntimeError(\"Unable to find plot ID for file %s\" % file_path)\n",
    "    return found_plot_id\n",
    "\n",
    "\n",
    "def local_get_save_files(local_folder: str, sensors: tuple, seasons: list, date_season_ids: dict,\n",
    "                         db_conn: sqlite3.Connection) -> dict:\n",
    "    \"\"\"Fetches file information associated with the sensors and dates from locally and updates the database\n",
    "    Arguments:\n",
    "        local_folder: the local endpoint to access\n",
    "        sensors: a tuple of sensors to work on\n",
    "        seasons: the list of seasons\n",
    "        date_season_ids: dates with their associated season ID\n",
    "        db_conn: the database to write to\n",
    "    Return:\n",
    "        Returns a dictionary of file IDs, and their associated start and finish timestamps as a tuple\n",
    "    Exceptions:\n",
    "        RuntimeError is raised if a problem is detected.\n",
    "        All caught exceptions are logged and re-raised\n",
    "    \"\"\"\n",
    "    files_timestamp = {}\n",
    "    \n",
    "    # Check that the path appears valid\n",
    "    if not os.path.exists(local_folder):\n",
    "        raise RuntimeError(\"Local folder does not exist or is not accessible: '%s'\" % (local_folder))\n",
    "\n",
    "    # Create the table for file information\n",
    "    file_cursor = db_conn.cursor()\n",
    "    file_cursor.execute('''CREATE TABLE files\n",
    "                          (id INTEGER, folder TEXT, filename TEXT, format TEXT, sensor TEXT, start_time TEXT, finish_time TEXT,\n",
    "                           gantry_x FLOAT, gantry_y FLOAT, gantry_z FLOAT, plot_id INTEGER, season_id INTEGER)''')\n",
    "\n",
    "    # Loop through each sensor and dates and get the associated file information\n",
    "    num_inserted = 0\n",
    "    total_records = 0\n",
    "    file_id = 1\n",
    "    try:\n",
    "        for one_sensor in sensors:\n",
    "            sensor = one_sensor\n",
    "            paths = SENSOR_MAPS[one_sensor]['file_paths']\n",
    "            for one_path in paths:\n",
    "                if SENSOR_MAPS[one_sensor]['metadata_file_mapper']:\n",
    "                    mfm = SENSOR_MAPS[one_sensor]['metadata_file_mapper']\n",
    "                else:\n",
    "                    mfm = None\n",
    "                filename_filter = None\n",
    "                if 'exclude_check' in one_path:\n",
    "                    filename_filter = one_path['exclude_check']\n",
    "                files = local_get_files(local_folder, one_path['path'], one_path['ext'], date_season_ids, mfm, filename_filter)\n",
    "                if not files:\n",
    "                    logging.warning(\"Unable to find files for dates for sensor %s\", sensor)\n",
    "                    continue\n",
    "\n",
    "                for one_date in files.keys():\n",
    "                    date_files = files[one_date]\n",
    "                    season_id = date_season_ids[one_date]\n",
    "                    for one_file in date_files:\n",
    "                        plot_id = map_file_to_plot_id(os.path.join(one_file['directory'], one_file['filename']), season_id, seasons)\n",
    "                        file_cursor.execute('INSERT INTO files VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "                                            [file_id, one_file['directory'], one_file['filename'], one_file['format'],\n",
    "                                             sensor, one_file['start_time'], one_file['finish_time'], one_file['gantry_x'],\n",
    "                                             one_file['gantry_y'], one_file['gantry_z'], plot_id, season_id])\n",
    "\n",
    "                        files_timestamp[file_id] = (make_timestamp_instance(one_file['start_time']),\n",
    "                                                    make_timestamp_instance(one_file['finish_time']))\n",
    "\n",
    "                        file_id += 1\n",
    "                        num_inserted += 1\n",
    "                        total_records += 1\n",
    "                        if num_inserted >= MAX_INSERT_BEFORE_COMMIT:\n",
    "                            db_conn.commit()\n",
    "                            num_inserted = 0\n",
    "\n",
    "    except Exception as ex:\n",
    "        logging.error(\"Exception caught in local_get_save_files: %s\", str(ex))\n",
    "        if logging.getLogger().level == logging.DEBUG:\n",
    "            logging.exception(ex)\n",
    "        raise ex\n",
    "\n",
    "    # Create the indexes\n",
    "    file_cursor.execute(\"CREATE UNIQUE INDEX 'files_index' on 'files' ('id', 'plot_id' ASC)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    file_cursor.close()\n",
    "\n",
    "    if total_records <= 0:\n",
    "        logging.warning(\"No file records were written\")\n",
    "    else:\n",
    "        logging.debug(\"Wrote %s file records\", str(total_records))\n",
    "\n",
    "    return files_timestamp\n",
    "\n",
    "\n",
    "def local_get_all_weather(dates: list) -> dict:\n",
    "    \"\"\"Returns a dictionary of all the weather found for the dates provided\n",
    "    Arguments:\n",
    "        dates: the list of dates to get\n",
    "    Return:\n",
    "        Returns a dictionary with dates as keys, each associated with a list of informational dict's on the weather for those dates\n",
    "    \"\"\"\n",
    "    found_weather = {}\n",
    "    base_path = os.path.join(LOCAL_START_PATH, LOCAL_ENVIRONMENT_LOGGER_PATH)\n",
    "\n",
    "    # Setup for getting files that aren't local\n",
    "    dates_files = {}\n",
    "    file_transfer_needed = False\n",
    "    for one_date in dates:\n",
    "        cur_path = os.path.join(base_path, one_date)\n",
    "        logging.debug(\"Local path: %s\", cur_path)\n",
    "        path_contents = local_folder_list(cur_path)\n",
    "        dates_files[one_date] = []\n",
    "        for one_entry in path_contents:\n",
    "            if one_entry['type'] == 'file':\n",
    "                json_path = os.path.join(cur_path, one_entry['name'])\n",
    "                logging.debug(\"Local file path: %s\", json_path)\n",
    "                dates_files[one_date].append(json_path)\n",
    "\n",
    "    # Loop through and load all the data\n",
    "    problems_found = False\n",
    "    for one_date, date_file_list in dates_files.items():\n",
    "        if date_file_list:\n",
    "            found_weather[one_date] = []\n",
    "            logging.debug(\"Loading %s weather files for date %s\", len(date_file_list), one_date)\n",
    "            for one_file in date_file_list:\n",
    "                with open(one_file, 'r') as in_file:\n",
    "                    weather = json.load(in_file)\n",
    "                    if 'environment_sensor_readings' in weather:\n",
    "                        for one_reading in weather['environment_sensor_readings']:\n",
    "                            weather_info = {'timestamp': one_reading['timestamp']}\n",
    "                            for one_sensor, sensor_readings in one_reading['weather_station'].items():\n",
    "                                weather_info[one_sensor] = sensor_readings['value']\n",
    "                            found_weather[one_date].append(weather_info)\n",
    "                    else:\n",
    "                        logging.error(\"Unknown JSON file format for weather file '%s'\", one_file)\n",
    "                        problems_found = True\n",
    "        else:\n",
    "            logging.debug(\"Found no files to load for date %s\", one_date)\n",
    "\n",
    "    if problems_found:\n",
    "        raise RuntimeError(\"Unable to complete loading weather data due to previous problems\")\n",
    "\n",
    "    return found_weather\n",
    "\n",
    "\n",
    "def get_save_weather(date_experiment_ids: dict, db_conn: sqlite3.Connection) -> dict:\n",
    "    \"\"\"Retrieves  and  saves weather  data\n",
    "    Arguments:\n",
    "        date_experiment_ids: dates with their associated experiment ID\n",
    "        db_conn: the database to write to\n",
    "    Return:\n",
    "        Returns a dict of the weather ID and its associated timestamp\n",
    "    \"\"\"\n",
    "    weather_timestamps = {}\n",
    "\n",
    "    # Create the table for file information\n",
    "    weather_cursor = db_conn.cursor()\n",
    "    weather_cursor.execute('''CREATE TABLE weather\n",
    "                           (id INTEGER, timestamp TEXT, temperature FLOAT, illuminance FLOAT, precipitation FLOAT, sun_direction FLOAT,\n",
    "                           wind_speed FLOAT, wind_direction FLOAT, relative_humidity FLOAT)''')\n",
    "\n",
    "    # Loop through each sensor and dates and get the associated file information\n",
    "    num_inserted = 0\n",
    "    total_records = 0\n",
    "    problems_found = 0\n",
    "    weather_id = 1\n",
    "    # Load all the data to be found and check for missing dates (aka: missing data) below\n",
    "    all_weather = local_get_all_weather(list(date_experiment_ids.keys()))\n",
    "    for one_date in date_experiment_ids:\n",
    "        if one_date not in all_weather:\n",
    "            logging.warning(\"Unable to find weather data for date %s\", one_date)\n",
    "            problems_found = True\n",
    "            continue\n",
    "\n",
    "        for one_weather in all_weather[one_date]:\n",
    "            weather_cursor.execute('INSERT INTO weather VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "                                   [weather_id, one_weather['timestamp'], one_weather['temperature'], one_weather['brightness'],\n",
    "                                    one_weather['precipitation'], one_weather['sunDirection'], one_weather['windVelocity'],\n",
    "                                    one_weather['windDirection'], one_weather['relHumidity']])\n",
    "\n",
    "            weather_timestamps[weather_id] = make_timestamp_instance(one_weather['timestamp'])\n",
    "\n",
    "            weather_id += 1\n",
    "            num_inserted += 1\n",
    "            total_records += 1\n",
    "            if num_inserted >= MAX_INSERT_BEFORE_COMMIT:\n",
    "                db_conn.commit()\n",
    "                num_inserted = 0\n",
    "\n",
    "    # Create the index\n",
    "    weather_cursor.execute(\"CREATE UNIQUE INDEX 'weather_index' ON 'weather' ('id' ASC)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    weather_cursor.close()\n",
    "\n",
    "    if problems_found:\n",
    "        logging.error(\"Unable to retrieve weather data for all dates\")\n",
    "\n",
    "    if total_records <= 0:\n",
    "        logging.warning(\"No weather records were written\")\n",
    "\n",
    "    logging.debug(\"Wrote %s weather records\", str(total_records))\n",
    "\n",
    "    return weather_timestamps\n",
    "\n",
    "\n",
    "def get_ordered_weather_ids_timestamps(weather_timestamps: dict) -> tuple:\n",
    "    \"\"\"Returns a tuple containing the ordered list of weather IDs and their associated timestamps\n",
    "    Arguments:\n",
    "        weather_timestamps: the dictionary of weather IDs and their timestamps\n",
    "    Return:\n",
    "        A tuple containing an ordered tuple of weather IDs and and ordered tuple of timestamps\n",
    "    \"\"\"\n",
    "    ids = list(weather_timestamps.keys())\n",
    "    tss = list(weather_timestamps.values())\n",
    "\n",
    "    ids.sort()\n",
    "    tss.sort()\n",
    "\n",
    "    return tuple(ids), tuple(tss)\n",
    "\n",
    "\n",
    "def find_file_weather_ids(start_ts: datetime, finish_ts: datetime, ordered_weather_ids: tuple, ordered_weather_timestamps: tuple) -> tuple:\n",
    "    \"\"\"Finds the minimum and maximum weather timestamps associated with the files start and finish timestamps\n",
    "    Arguments:\n",
    "        start_ts: the starting timestamp to look for\n",
    "        finish_ts: the finishing timestamp to look for\n",
    "        ordered_weather_ids: the ordered list of weather IDs\n",
    "        ordered_weather_timestamps: the ordered list of timestamps\n",
    "    Return:\n",
    "        A tuple containing the ID of the starting and ending weather timestamps that encompass the file's timestamps\n",
    "    Notes:\n",
    "        Assumes the ascending numerical order of weather IDs are directly related to the ascending temporal order of\n",
    "        the weather timestamp (in other words a larger ID value occurs later than any of the smaller ID values)\n",
    "    \"\"\"\n",
    "    assert len(ordered_weather_ids) == len(ordered_weather_timestamps)\n",
    "\n",
    "    def b_search(search_timestamp: datetime, ordered_timestamps: tuple) -> tuple:\n",
    "        \"\"\"Find the nearest min and max timestamp indexes for the specified search timestamp\n",
    "        Arguments:\n",
    "            search_timestamp: the timestamp to look for\n",
    "            ordered_timestamps: the ordered list of timestamps to search\n",
    "        Return:\n",
    "            A tuple containing the min and max indexes encompassing the timestamp\n",
    "        Notes:\n",
    "            If a timestamp index can't be found, None is returned in the tuple\n",
    "        \"\"\"\n",
    "        min_index, max_index = None, None\n",
    "\n",
    "        first_idx = 0\n",
    "        last_idx = len(ordered_timestamps) - 1\n",
    "\n",
    "        # Simple cases first (empty tuple, one element tuple)\n",
    "        if last_idx < first_idx:\n",
    "            return min_index, max_index\n",
    "        if first_idx == last_idx:\n",
    "            if ordered_timestamps[first_idx] == search_timestamp:\n",
    "                min_index, max_index = first_idx, last_idx\n",
    "            elif ordered_timestamps[first_idx] < search_timestamp:\n",
    "                min_index = first_idx\n",
    "            else:\n",
    "                max_index = last_idx\n",
    "            return min_index, max_index\n",
    "\n",
    "        # Check the cases the loop doesn't handle\n",
    "        if ordered_timestamps[last_idx] == search_timestamp:\n",
    "            return last_idx, last_idx\n",
    "        if ordered_timestamps[last_idx] < search_timestamp:\n",
    "            return last_idx, None\n",
    "\n",
    "        # Perform search\n",
    "        while True:\n",
    "            mid_idx = int((first_idx + last_idx) / 2)\n",
    "            if ordered_timestamps[mid_idx] == search_timestamp:\n",
    "                min_index, max_index = mid_idx, mid_idx\n",
    "                break\n",
    "            if ordered_timestamps[mid_idx] < search_timestamp:\n",
    "                first_idx = mid_idx\n",
    "            else:\n",
    "                last_idx = mid_idx\n",
    "            if last_idx - first_idx <= 1:\n",
    "                if ordered_timestamps[first_idx] < search_timestamp:\n",
    "                    min_index = first_idx\n",
    "                if ordered_timestamps[last_idx] > search_timestamp:\n",
    "                    max_index = last_idx\n",
    "                break\n",
    "\n",
    "        return min_index, max_index\n",
    "\n",
    "    min_start_index, max_start_index = b_search(start_ts, ordered_weather_timestamps)\n",
    "    min_finish_index, max_finish_index = b_search(finish_ts, ordered_weather_timestamps)\n",
    "    if None in (min_start_index, max_start_index, min_finish_index, max_finish_index):\n",
    "        raise RuntimeError(\"Unable to find weather associated with file timestamps: %s %s\" % (start_ts, finish_ts))\n",
    "    if min_start_index > min_finish_index:\n",
    "        raise RuntimeError(\"Something went horribly wrong finding weather associated with file timestamps: %s %s\" % (start_ts, finish_ts))\n",
    "\n",
    "    start_index = min_start_index\n",
    "    finish_index = max_finish_index\n",
    "    # Return the closest weather (comment out the next few lines to keep the bracketing weather)\n",
    "    if abs((start_ts - ordered_weather_timestamps[min_start_index]).total_seconds()) > \\\n",
    "            abs((start_ts - ordered_weather_timestamps[max_start_index]).total_seconds()):\n",
    "        start_index = max_start_index\n",
    "    if abs((finish_ts - ordered_weather_timestamps[min_finish_index]).total_seconds()) < \\\n",
    "            abs((finish_ts - ordered_weather_timestamps[max_finish_index]).total_seconds()):\n",
    "        finish_index = min_finish_index\n",
    "\n",
    "    return ordered_weather_ids[start_index], ordered_weather_ids[finish_index]\n",
    "\n",
    "\n",
    "def create_weather_files_table(weather_timestamps: dict, files_timestamps: dict, db_conn: sqlite3.Connection) -> None:\n",
    "    \"\"\"Creates a mapping table between the weather and files\n",
    "    Arguments:\n",
    "        weather_timestamps: a dictionary of the weather IDs and their timestamp\n",
    "        files_timestamps: a dictionary of the file IDs and their starting and finishing timestamps\n",
    "        db_conn: the database to write to\n",
    "    \"\"\"\n",
    "    # Create the table for file information\n",
    "    wf_cursor = db_conn.cursor()\n",
    "    wf_cursor.execute('''CREATE TABLE weather_file_map\n",
    "                           (id INTEGER, file_id INTEGER, min_weather_id INTEGER, max_weather_id INTEGER)''')\n",
    "\n",
    "    # Loop through each sensor and dates and get the associated file information\n",
    "    num_inserted = 0\n",
    "    total_records = 0\n",
    "    problems_found = 0\n",
    "    wf_id = 1\n",
    "\n",
    "    ordered_weather_ids, ordered_weather_timestamps = get_ordered_weather_ids_timestamps(weather_timestamps)\n",
    "    logging.info(\"Looking up %s files for their associated weather\", str(len(files_timestamps)))\n",
    "    for file_id, file_start_finish_ts in files_timestamps.items():\n",
    "        min_weather_id, max_weather_id = find_file_weather_ids(file_start_finish_ts[0], file_start_finish_ts[1],\n",
    "                                                               ordered_weather_ids, ordered_weather_timestamps)\n",
    "        wf_cursor.execute('INSERT INTO weather_file_map VALUES(?, ?, ?, ?)', [wf_id, file_id, min_weather_id, max_weather_id])\n",
    "        wf_id += 1\n",
    "        num_inserted += 1\n",
    "        total_records += 1\n",
    "        if num_inserted >= MAX_INSERT_BEFORE_COMMIT:\n",
    "            db_conn.commit()\n",
    "            num_inserted = 0\n",
    "\n",
    "    # Create the index\n",
    "    wf_cursor.execute(\"CREATE UNIQUE INDEX 'weather_file_map_index' ON 'weather_file_map' ('id' ASC)\")\n",
    "    wf_cursor.execute(\"CREATE INDEX 'weather_file_map_lookup_index' ON 'weather_file_map' ('min_weather_id', 'max_weather_id' ASC)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    wf_cursor.close()\n",
    "\n",
    "    if problems_found:\n",
    "        raise RuntimeError(\"Unable to retrieve weather data for all dates\")\n",
    "\n",
    "    if total_records <= 0:\n",
    "        logging.warning(\"No weather records were written\")\n",
    "\n",
    "    logging.debug(\"Wrote %s weather files mapping records\", str(total_records))\n",
    "\n",
    "\n",
    "def save_gene_markers(gene_marker_file: str, key_column_index: int, file_row_ignore: int, db_conn: sqlite3.Connection) -> dict:\n",
    "    \"\"\"Saves the gene marker file into the database\n",
    "    Arguments:\n",
    "        gene_marker_file: path to the gene marker file to import\n",
    "        key_column_index: the index of the column to provide key values\n",
    "        file_row_ignore: number of rows to ignore at the start of the file\n",
    "        db_conn: the database to write to\n",
    "    Return:\n",
    "        Returns a dictionary of row IDs and the key value\n",
    "    \"\"\"\n",
    "    if not key_column_index:\n",
    "        key_index = 0\n",
    "    else:\n",
    "        key_index = int(key_column_index)\n",
    "    if not file_row_ignore:\n",
    "        skip_count = 0\n",
    "    else:\n",
    "        skip_count = int(file_row_ignore)\n",
    "\n",
    "    gene_cursor = db_conn.cursor()\n",
    "\n",
    "    id_key_map = {}\n",
    "    created_table = False\n",
    "    column_order = None\n",
    "    insert_sql = None\n",
    "    rows_inserted = 0\n",
    "    with open(gene_marker_file, 'r') as in_file:\n",
    "        # Skip over the rows as requested\n",
    "        if skip_count:\n",
    "            logging.info('Skipping %s rows at start of gene marker file: %s', str(skip_count), gene_marker_file)\n",
    "        while skip_count > 0:\n",
    "            skipped_line = in_file.readline()\n",
    "            logging.debug(\"Skipping line: %s\", skipped_line)\n",
    "            skip_count -= 1\n",
    "\n",
    "        # Process the rest of the file\n",
    "        reader = csv.DictReader(in_file)\n",
    "        row_id = 1\n",
    "        for row in reader:\n",
    "            # Create the table the first time through\n",
    "            if not created_table:\n",
    "                column_order = tuple(row.keys())\n",
    "                if key_index >= len(column_order):\n",
    "                    raise RuntimeError('Gene mapping key column index value (%s) is greater than the number of columns: %s' %\n",
    "                                       (str(key_index), str(len(column_order))))\n",
    "                column_names = tuple([column.replace(' ', '_').replace('.', '_').lower() for column in column_order])\n",
    "                logging.info('Creating gene_markers table with columns: %s', str(column_names))\n",
    "                create_sql = 'CREATE TABLE gene_markers (%s)' % ('id INTEGER, ' + ' TEXT, '.join(column_names) + ' TEXT')\n",
    "                logging.debug('Create gene_markers SQL: %s', create_sql)\n",
    "                gene_cursor.execute(create_sql)\n",
    "                insert_sql = 'INSERT INTO gene_markers(id, ' + ','.join(column_names) + ') VALUES(' + \\\n",
    "                                   ','.join(['?' for _ in range(0, len(column_names) + 1)]) + ')'\n",
    "                logging.debug('Insert gene_markers SQL: %s', insert_sql)\n",
    "                created_table = True\n",
    "\n",
    "            # Add the row\n",
    "            insert_values = [row_id]\n",
    "            for one_column in column_order:\n",
    "                insert_values.append(row[one_column])\n",
    "            gene_cursor.execute(insert_sql, insert_values)\n",
    "            id_key_map[row_id] = row[column_order[key_index]]\n",
    "            rows_inserted += 1\n",
    "            row_id += 1\n",
    "\n",
    "    # Create the index\n",
    "    gene_cursor.execute(\"CREATE UNIQUE INDEX 'gene_markers_index' ON 'gene_markers' ('id' ASC)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    gene_cursor.close()\n",
    "\n",
    "    if not created_table:\n",
    "        raise RuntimeError(\"Empty gene marker file specified\")\n",
    "    logging.info(\"Inserted %s rows into gene marker table\", str(rows_inserted))\n",
    "\n",
    "    return id_key_map\n",
    "\n",
    "\n",
    "def save_cultivar_genes(cultivar_gene_file: str, key_column_index: int, file_row_ignore: int, db_conn: sqlite3.Connection) -> tuple:\n",
    "    \"\"\"Saves the cultivar to genes file into the database\n",
    "    Arguments:\n",
    "        cultivar_gene_file: path to the cultivar gene file to import\n",
    "        key_column_index: the index of the column to provide key values\n",
    "        file_row_ignore: number of rows to ignore at the start of the file\n",
    "        db_conn: the database to write to\n",
    "    Return:\n",
    "        Returns the a tuple containing the column name of the cultivar field, and a list of table columns from the file\n",
    "    \"\"\"\n",
    "    if not key_column_index:\n",
    "        key_index = 0\n",
    "    else:\n",
    "        key_index = int(key_column_index)\n",
    "    if not file_row_ignore:\n",
    "        skip_count = 0\n",
    "    else:\n",
    "        skip_count = int(file_row_ignore)\n",
    "\n",
    "    cg_cursor = db_conn.cursor()\n",
    "\n",
    "    cultivar_column_name = None\n",
    "    created_table = False\n",
    "    column_order = None\n",
    "    column_names = None\n",
    "    insert_sql = None\n",
    "    rows_inserted = 0\n",
    "    with open(cultivar_gene_file, 'r') as in_file:\n",
    "        # Skip over the rows as requested\n",
    "        if skip_count:\n",
    "            logging.info('Skipping %s rows at start of cultivar_gene file: %s', str(skip_count), cultivar_gene_file)\n",
    "        while skip_count > 0:\n",
    "            skipped_line = in_file.readline()\n",
    "            logging.debug(\"Skipping line: %s\", skipped_line)\n",
    "            skip_count -= 1\n",
    "\n",
    "        # Process the rest of the file\n",
    "        reader = csv.DictReader(in_file)\n",
    "        row_id = 1\n",
    "        for row in reader:\n",
    "            # Create the table the first time through\n",
    "            if not created_table:\n",
    "                column_order = tuple(row.keys())\n",
    "                if key_index >= len(column_order):\n",
    "                    raise RuntimeError('Cultivar gene key column index value (%s) is greater than the number of columns: %s' %\n",
    "                                       (str(key_index), str(len(column_order))))\n",
    "                column_names = tuple([column.replace(' ', '_').replace('.', '_').lower() for column in column_order])\n",
    "                cultivar_column_name = column_names[key_index]\n",
    "                logging.debug(\"Cultivar column name for cultivar_genes table: %s\", cultivar_column_name)\n",
    "                logging.info('Creating cultivar_genes table with columns: %s', str(column_names))\n",
    "                create_sql = 'CREATE TABLE cultivar_genes (%s)' %\\\n",
    "                             ('id INTEGER, ' + column_names[0] + ' TEXT, ' + ' INTEGER, '.join(column_names[1:]) + ' INTEGER')\n",
    "                logging.debug('Create cultivar_genes SQL: %s', create_sql)\n",
    "                cg_cursor.execute(create_sql)\n",
    "                insert_sql = 'INSERT INTO cultivar_genes(id, ' + ','.join(column_names) + ') VALUES(' + \\\n",
    "                                   ','.join(['?' for _ in range(0, len(column_names) + 1)]) + ')'\n",
    "                logging.debug('Insert cultivar_genes SQL: %s', insert_sql)\n",
    "                created_table = True\n",
    "\n",
    "            # Add the row\n",
    "            insert_values = [row_id]\n",
    "            for one_column in column_order:\n",
    "                int_match = re.search('^[-+]?\\\\d+$', row[one_column])\n",
    "                if row[one_column] == 'No WGS':\n",
    "                    insert_values.append(-1)\n",
    "                elif row[one_column] == 'NA':\n",
    "                    insert_values.append(-2)\n",
    "                elif int_match is not None:\n",
    "                    insert_values.append(int(row[one_column]))\n",
    "                else:\n",
    "                    insert_values.append(row[one_column])\n",
    "            cg_cursor.execute(insert_sql, insert_values)\n",
    "            rows_inserted += 1\n",
    "            row_id += 1\n",
    "\n",
    "    # Create the index\n",
    "    cg_cursor.execute(\"CREATE UNIQUE INDEX 'cultivar_genes_index' ON 'cultivar_genes' ('id','\" + cultivar_column_name + \"' ASC)\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    cg_cursor.close()\n",
    "\n",
    "    if not created_table:\n",
    "        raise RuntimeError(\"Empty cultivar genes file specified\")\n",
    "    logging.info(\"Inserted %s rows into cultivar genes table\", str(rows_inserted))\n",
    "\n",
    "    return cultivar_column_name, column_names\n",
    "\n",
    "\n",
    "def create_db_views(db_conn: sqlite3.Connection, cultivar_genes_cultivar_column_name: str,\n",
    "                    cultivar_genes_all_column_names: list) -> None:\n",
    "    \"\"\"Adds views to the database\n",
    "    Arguments:\n",
    "        db_conn: the database to write to\n",
    "        cultivar_genes_cultivar_column_name: the column name in the cultivar_genes table that contains the cultivars\n",
    "        cultivar_genes_all_column_names: the list of all column names in the cultivar_genes table\n",
    "    \"\"\"\n",
    "    view_cursor = db_conn.cursor()\n",
    "\n",
    "    view_cursor.execute('''CREATE VIEW cultivar_files AS select e.id as plot_id, e.plot_name as plot_name, e.season as season,\n",
    "                        e.plot_bb_min_lat as plot_bb_min_lat, e.plot_bb_min_lon as plot_bb_min_lon,\n",
    "                        e.plot_bb_max_lat as plot_bb_max_lat, e.plot_bb_max_lon as plot_bb_max_lon,\n",
    "                        f.id as file_id, f.folder as folder, f.filename as filename, f.format as format, f.sensor as sensor,\n",
    "                        f.start_time as start_time, f.finish_time as finish_time, f.gantry_x as gantry_x, f.gantry_y as gantry_y,\n",
    "                        f.gantry_z as gantry_z, c.name as cultivar_name\n",
    "                        from season_info as e left join files as f on e.id = f.plot_id \n",
    "                            left join cultivars as c on e.cultivar_id = c.id''')\n",
    "\n",
    "    view_cursor.execute('''CREATE VIEW weather_files AS select * from (select w.timestamp as timestamp, w.temperature as temperature,\n",
    "                        w.illuminance as illuminance, w.precipitation as precipitation, w.sun_direction as sun_direction,\n",
    "                        w.wind_speed as wind_speed, w.wind_direction as wind_direction, w.relative_humidity as relative_humidity, \n",
    "                        f.id as file_id, f.folder as folder, f.filename as filename, f.format as format, f.sensor as sensor,\n",
    "                        f.start_time as start_time, f.finish_time as finish_time, f.gantry_x as gantry_x, f.gantry_y as gantry_y,\n",
    "                        f.gantry_z as gantry_z\n",
    "                        from weather as w left join weather_file_map as wf on w.id = wf.min_weather_id\n",
    "                            left join files as f on wf.file_id = f.id) a where not a.file_id is NULL''')\n",
    "\n",
    "    view_template = '''CREATE VIEW unified as select f.id as file_id, f.folder as folder, f.filename as filename,\n",
    "                    f.format as format, f.sensor as sensor, f.start_time as start_time, f.finish_time as finish_time,\n",
    "                    f.gantry_x as gantry_x, f.gantry_y as gantry_y, f.gantry_z as gantry_z,\n",
    "                    e.id as plot_id, e.plot_name as plot_name, e.season as season,\n",
    "                    e.plot_bb_min_lat as plot_bb_min_lat, e.plot_bb_min_lon as plot_bb_min_lon,\n",
    "                    e.plot_bb_max_lat as plot_bb_max_lat, e.plot_bb_max_lon as plot_bb_max_lon,\n",
    "                    c.name as cultivar_name,\n",
    "                    %s\n",
    "                    w.timestamp as weather_timestamp, w.temperature as temperature,\n",
    "                    w.illuminance as illuminance, w.precipitation as precipitation, w.sun_direction as sun_direction,\n",
    "                    w.wind_speed as wind_speed, w.wind_direction as wind_direction, w.relative_humidity as relative_humidity\n",
    "                    from files f left join season_info as e on f.plot_id = e.id\n",
    "                        left join cultivars as c on e.cultivar_id = c.id\n",
    "                        %s\n",
    "                        left join weather_files as w on f.id = w.file_id'''\n",
    "\n",
    "    if cultivar_genes_cultivar_column_name:\n",
    "        join_columns = ['cg.' + one_name for one_name in cultivar_genes_all_column_names\n",
    "                        if one_name not in ['id', cultivar_genes_cultivar_column_name]]\n",
    "        view_sql = view_template % (','.join(join_columns) + ', ', 'left join cultivar_genes as cg on c.name = cg.' +\n",
    "                                    cultivar_genes_cultivar_column_name)\n",
    "    else:\n",
    "        view_sql = view_template % ('', '')\n",
    "    logging.debug('Unified view SQL: %s', view_sql)\n",
    "    view_cursor.execute(view_sql)\n",
    "\n",
    "    view_cursor.close()\n",
    "\n",
    "\n",
    "def count_final_records(db_conn: sqlite3.Connection) -> int:\n",
    "    \"\"\"Adds views to the database\n",
    "    Arguments:\n",
    "        db_conn: the database to query\n",
    "    Return:\n",
    "        Returns the final number of records in the combined view\n",
    "    \"\"\"\n",
    "    count_cursor = db_conn.cursor()\n",
    "\n",
    "    count_sql = '''SELECT count(1) FROM unified'''\n",
    "    count_cursor.execute(count_sql)\n",
    "\n",
    "    count = count_cursor.fetchone()\n",
    "\n",
    "    if count:\n",
    "        return int(count[0])\n",
    "\n",
    "    return 0\n",
    "\n",
    "def generate() -> None:\n",
    "    \"\"\"Performs all the steps needed to generate the SQLite database\n",
    "    Exceptions:\n",
    "        RuntimeError exceptions are raised when something goes wrong\n",
    "    \"\"\"\n",
    "    # Check for debugging\n",
    "    if LOG_INFO:\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "    if LOG_DEBUG:\n",
    "        logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    # Break apart any command line arguments that may be multi-part\n",
    "    sensors = prepare_sensors(SENSORS)\n",
    "    dates = prepare_dates(DATES)\n",
    "    logging.info(\"Specified sensors: %s\", str(sensors))\n",
    "    logging.info(\"Specified %s dates: %s\", str(len(dates)), str(dates))\n",
    "\n",
    "    # Get other values we'll need\n",
    "    betydb_url = get_betydb_url(BETYDB_URL)\n",
    "    betydb_key = get_betydb_key(BETYDB_KEY)\n",
    "    brapi_url = get_brapi_url(BRAPI_URL)\n",
    "\n",
    "    # Get our temporary file name\n",
    "    _, working_filename = tempfile.mkstemp()\n",
    "    sql_db = sqlite3.connect(working_filename)\n",
    "\n",
    "    try:\n",
    "        # Generate the experiments table\n",
    "        experiments, cultivars, date_experiment_ids = get_save_experiments(dates, sql_db, betydb_url, betydb_key, brapi_url)\n",
    "\n",
    "        # Generating the cultivars table\n",
    "        save_cultivars(cultivars, sql_db)\n",
    "\n",
    "        # Create the files table\n",
    "        files_timestamps = local_get_save_files(LOCAL_START_PATH, sensors, experiments, date_experiment_ids, sql_db)\n",
    "\n",
    "        # Create the weather table\n",
    "        weather_timestamps = get_save_weather(date_experiment_ids, sql_db)\n",
    "\n",
    "        # Create supporting tables\n",
    "        create_weather_files_table(weather_timestamps, files_timestamps, sql_db)\n",
    "\n",
    "        # Add gene marker information\n",
    "        gene_markers_map = None\n",
    "        cultivar_column_name = None\n",
    "        cultivar_genes_column_names = None\n",
    "#        if args.gene_marker_file:\n",
    "#            gene_markers_map = save_gene_markers(args.gene_marker_file, args.gene_marker_file_key, args.gene_marker_file_ignore, sql_db)\n",
    "#        if args.cultivar_gene_map_file:\n",
    "#            cultivar_column_name, cultivar_genes_column_names = save_cultivar_genes(args.cultivar_gene_map_file, args.cultivar_gene_file_key,\n",
    "#                                                                                    args.cultivar_gene_map_file_ignore, sql_db)\n",
    "\n",
    "        # Create the views\n",
    "        create_db_views(sql_db, cultivar_column_name, cultivar_genes_column_names)\n",
    "\n",
    "        # Count the number of final records\n",
    "        final_count = count_final_records(sql_db)\n",
    "        if final_count:\n",
    "            logging.info(\"Records available: %s\", str(final_count))\n",
    "        else:\n",
    "            logging.warning(\"No records are available\")\n",
    "\n",
    "        sql_db.close()\n",
    "        shutil.move(working_filename, OUTPUT_FILE)\n",
    "        sql_db = None\n",
    "    finally:\n",
    "        if sql_db:\n",
    "            sql_db.close()\n",
    "        del sql_db\n",
    "        if os.path.exists(working_filename):\n",
    "            os.unlink(working_filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
